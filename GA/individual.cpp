#include "individual.h"

#include <iostream>
#include <random>

individual::individual(vector<int> *topology)
{
  this->topology = topology;
}

// Initializing solution vectors with pseudo-random values with selected distribution.
// Note that solution is one layer smaller than neural net (solution doesn't have output
// layer, as it doesn't have weights."
void individual::initialize()
{
  // Check if topology was set. If not log error and die return;
  if(topology == NULL)
  {
    cout << "\nERROR: Topology was not set!\n";
    return;
  }

	// Using exponential_distribution because distribution of weights in neural net is meant
	// to resemble one achieved by backpropagation (works by D. Montana).

	std::default_random_engine generator;
	std::exponential_distribution<double> distribution(lambda);

  // Topology size is number of layers, and value of each element is number of neurons
  // on this layer. Solution can be constructed thanks to that information.

  for(unsigned int i = 0; i < topology->size()-1; ++i)
  {
    //For each layer add empty neurons.
    solution.push_back(neurons());

    for(unsigned int j = 0; j < topology->at(i); ++j)
    {
      // For that neuron add weights.
      solution.at(i).push_back(weights());

      for(unsigned int k = 0; k < topology->at(i+1); ++k)
        solution.at(i).at(j).push_back(randomlyTurnNegative(distribution(generator)));
    }
  }
}

// Select mutation method(s) here.
// nodeMutation is preferred as it's the most effective one.
void individual::mutate()
{
  nodeMutation();
}

// For each entry in the chromosome, this operator will with fixed probability
// replace it with a random value chosen from the initialization probability
// distribution.
void individual::unbiasedMutation()
{
  std::default_random_engine generator;
  std::exponential_distribution<double> distribution(lambda);

  // For each layer.
  for(unsigned int i = 0; i < solution.size(); ++i)
  {
    // For each neuron.
    for(unsigned int j = 0; j < solution.at(i).size(); ++j)
    {
      // For each weight.
      for(unsigned int k = 0; k < solution.at(i).at(j).size(); ++k)
      {
        // If mutation occurred select new weight value;
        if(mutationOccurred()) solution.at(i).at(j).at(k) =
                                      randomlyTurnNegative(distribution(generator));
      }
    }
  }
}

// For each entry in the chromosome, this operator will with fixed probability
// add to it a random value chosen from the initialization probability distribution.
void individual::biasedMutation()
{
  std::default_random_engine generator;
  std::exponential_distribution<double> distribution(lambda);

  // For each layer.
  for(unsigned int i = 0; i < solution.size(); ++i)
  {
    // For each neuron.
    for(unsigned int j = 0; j < solution.at(i).size(); ++j)
    {
      // For each weight.
      for(unsigned int k = 0; k < solution.at(i).at(j).size(); ++k)
      {
        // If mutation occurred select add random value from selected distribution
        // to current value;
        if(mutationOccurred()) solution.at(i).at(j).at(k) +=
                                      randomlyTurnNegative(distribution(generator));
      }
    }
  }
}

// This operator selects n non-input nodes of the network which the parent chromosome
// represents. For each of the ingoing links to these n nodes, the operator adds to
// the links weight a random value from the initialization probability distribution.
void individual::nodeMutation()
{
  int layerNumber;
  int neuronNumber;

  std::default_random_engine generator;
  std::exponential_distribution<double> distribution(lambda);

  // For each node to mutate.
  for(unsigned int i = 0; i < numberOfNodesToMutate; ++i)
  {
    // Select random layer and position in the layer.
    layerNumber   = rand() % solution.size();
    neuronNumber  = rand() % solution.at(layerNumber).size();

    // For each weight of selected neuron
    for(unsigned int j = 0; j < solution.at(layerNumber).at(neuronNumber).size(); ++j)
    {
      // Add pseudo-random number generated by selected distribution.
      solution.at(layerNumber).at(neuronNumber).at(j) +=
              randomlyTurnNegative(distribution(generator));
    }
  }
}

// Select crossing metohd(s) here.
individual individual::cross(const individual parent)
{
  individual child(topology);


  return child;
}


void individual::setEvaluationValue(const double val)
{
	evaluationValue = val;
}

double individual::getEvaluationValue()
{
  return evaluationValue;
}

// For debug purposes.
string individual::toString()
{
  string result = "Solution: \n";

  for(unsigned int i = 0; i < solution.size(); ++i)
  {
    for (unsigned int j = 0; j < solution.at(i).size(); ++j)
    {
      result += "Layer " + to_string(i) + ", Neuron " + to_string(j) + ", Weights: ";

      for(unsigned int k = 0; k < solution.at(i).at(j).size(); ++k)
        result += to_string(solution.at(i).at(j).at(k)) + ", ";

      // Pop coma and space bar.
      result.pop_back();
      result.pop_back();

      result += "\n";
    }

    result += "\n";
  }

  return result;
}

// 50% chance to negate passed number.
double individual::randomlyTurnNegative(const double n)
{
  return (rand()%2 == 0) ? n : -n;
}

bool individual::mutationOccurred()
{
  return  (rand() % 100) < chromosomeMutationProbabilityPercent;
}